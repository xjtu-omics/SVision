#!/usr/bin/env python3

import sys
import os
import logging
import numpy as np
from time import strftime, localtime
# for file in os.listdir(os.getcwd()):
#     if os.path.isdir(file):
#         sys.path.append(os.path.join(os.getcwd(), file))

from src.collection import run_collection
from src.network.predict import Predict

import datetime
from src.network.output import merge_split_vcfs, cluster_original_callset, cal_scores_max_min
import shutil
import pysam
from src.collection.graph import collect_csv_same_format
from src.version import __version__

import argparse
import multiprocessing
import traceback


def parse_arguments(arguments = sys.argv[1:]):
    parser = argparse.ArgumentParser(formatter_class=argparse.RawDescriptionHelpFormatter,
                                     description="""SVision {0} \n \nShort Usage: SVision [parameters] -o <output path> -b <input bam path> -g <reference> -m <model path>""".format(__version__))


    required_params = parser.add_argument_group("Input/Output parameters")
    required_params.add_argument('-o', dest="out_path", type=os.path.abspath, required=True, help='Absolute path to output ')
    required_params.add_argument('-b', dest='bam_path', type=os.path.abspath, required=True, help='Absolute path to bam file')
    required_params.add_argument('-m', dest="model_path", type=os.path.abspath, required=True, help='Absolute path to CNN predict model')
    required_params.add_argument('-g', dest='genome', type=os.path.abspath, required=True, help='Absolute path to your reference genome (.fai required in the directory)')
    required_params.add_argument('-n', dest='sample', type=str, required=True, help='Name of the BAM sample name')


    optional_params = parser.add_argument_group("Optional parameters")
    optional_params.add_argument('-t', dest="thread_num", type=int, default=1, help='Thread numbers (default: %(default)s)')
    optional_params.add_argument('-s', dest="min_support", type=int, default=5, help='Minimum support read number required for SV calling (default: %(default)s)')
    optional_params.add_argument('-c', dest="chrom", type=str, default=None, help='Specific region (chr1:xxx-xxx) or chromosome (chr1) to detect')

    optional_params.add_argument('--hash', action="store_true", default=False,
                                 help='Activate local realignment for unmapped sequences (default: %(default)s)')

    # optional_params.add_argument('--cluster', action="store_true", default=False,
    #                              help='Cluster calls that might occur together (default: %(default)s)')

    optional_params.add_argument('--qname', action="store_true", default=False,
                                help='Report support names for each events (default: %(default)s)')

    optional_params.add_argument('--graph', action="store_true", default=False,
                                 help='Report graph for events (default: %(default)s)')

    optional_params.add_argument('--contig', action="store_true", default=False,
                                 help='Activate contig mode (default: %(default)s)')

    optional_params.add_argument('--debug', action="store_true", default=False,
                                 help='Activate debug mode and keep intermedia outputs (default: %(default)s)')


    # general_params.add_argument('--mechanism', action="store_true", default=False,
    #                              help='Report mechanisms for DEL event')
    # general_params.add_argument('--rpmask', type=os.path.abspath, default='repeatmasker',
    #                              help='Path to RepeatMasker')
    # general_params.add_argument('--trf', type=os.path.abspath, default='trf',
    #                              help='Path to TRF')

    collect_params = parser.add_argument_group("Collect parameters")

    collect_params.add_argument("--min_mapq", type=int, default=10, help='Minimum mapping quality of reads to consider (default: %(default)s)')
    collect_params.add_argument("--min_sv_size", type=int, default=50, help='Minimum SV size to detect (default: %(default)s)')
    collect_params.add_argument("--max_sv_size", type=int, default=1000000, help='Maximum SV size to detect (default: %(default)s)')
    collect_params.add_argument("--window_size", type=int, default=10000000, help='The sliding window size in segment collection (default: %(default)s)')


    cluster_params = parser.add_argument_group("Cluster parameters")
    cluster_params.add_argument("--patition_max_distance", type=int, default=5000,
                                help='Maximum distance to partition signatures (default: %(default)s)')
    cluster_params.add_argument("--cluster_max_distance", type=float, default=0.3,
                                help='Clustering maximum distance for a partition (default: %(default)s)')



    predict_params = parser.add_argument_group("Predict parameters")
    predict_params.add_argument("--batch_size", type=int, default=128, help='Batch size for the CNN prediction model (default: %(default)s)')

    genotype_params = parser.add_argument_group("Genotype parameters")
    genotype_params.add_argument('--min_gt_depth',type=int, default=4, help='Minimum reads required for genotyping (default: %(default)s)')
    genotype_params.add_argument('--homo_thresh', type=float, default=0.8,
                                 help='Minimum variant allele frequency to be called as homozygous (default: %(default)s)')
    genotype_params.add_argument('--hete_thresh', type=float, default=0.2,
                                 help='Minimum variant allele frequency to be called as heterozygous (default: %(default)s)')

    hash_params = parser.add_argument_group("Hash table parameters")
    hash_params.add_argument("--k_size", type=int, default=10, help='Size of kmer (default: %(default)s)')
    hash_params.add_argument("--min_accept", type=int, default=50,
                             help='Minimum match length for realignment (default: %(default)s)')
    hash_params.add_argument("--max_hash_len", type=int, default=1000,
                             help='Maximum length of unmapped sequence length for realignment (default: %(default)s)')

    options = parser.parse_args(arguments)

    return options


if __name__ == '__main__':
    options = parse_arguments()

    work_dir = options.out_path
    if not os.path.exists(work_dir):
        print(f'Create the output directory {work_dir}')
        os.mkdir(work_dir)

    ## v1.3.5 Create new log file
    log_format = logging.Formatter("%(asctime)s [%(levelname)-7.7s]  %(message)s")
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)


    fileHandler = logging.FileHandler("{0}/SVision_{1}.log".format(work_dir, strftime("%y%m%d_%H%M%S", localtime())), mode="w")
    fileHandler.setFormatter(log_format)

    consoleHandler = logging.StreamHandler()
    consoleHandler.setFormatter(log_format)
    root_logger.addHandler(fileHandler)
    ## End ADD

    logging.info('******************** Start SVision, version {0} ********************'.format(__version__))
    logging.info("CMD: {0}".format(" ".join(sys.argv)))
    logging.info("WORKDIR DIR: {0}".format(os.path.abspath(work_dir)))
    logging.info("CNN MODEL: {0}".format(os.path.abspath(options.model_path)))

    # # SVision v1.0.3. ADD. log file
    # log_file = open(os.path.join(work_dir, 'log.txt'), 'w')
    # # End ADD

    sample_path = options.bam_path
    aln_file = pysam.AlignmentFile(sample_path)

    ## Check the input
    logging.info("INPUT BAM: {0}".format(os.path.abspath(sample_path)))
    try:
        if aln_file.header["HD"]["SO"] == "coordinate":
            try:
                aln_file.check_index()
            except ValueError:
                logging.warning("Input BAM file is missing a valid index. Please generate with 'samtools faidx'. Continuing without genotyping for now..")
                options.skip_genotyping = True
            except AttributeError:
                logging.warning("pysam's .check_index raised an Attribute error. Something is wrong with the input BAM file.")
                exit()
    except:
        logging.error("This is not a coordinate sorted BAM file")
        exit()

    window_size = options.window_size

    if options.contig:
        options.min_support = 1

    task_list_bychrom = {}
    ref_info = aln_file.get_index_statistics()

    all_possible_chrs = pysam.FastaFile(options.genome).references

    if options.chrom == None:

        ## V1.3.6 add for multiprocessing
        for ele in ref_info:
            chrom = ele[0]
            local_ref_len = aln_file.get_reference_length(chrom)

            if chrom not in all_possible_chrs:
                continue

            if options.contig:
                window_size = local_ref_len

            if local_ref_len < window_size:
                if chrom in task_list_bychrom:
                    task_list_bychrom[chrom].append([0, local_ref_len])
                else:
                    task_list_bychrom[chrom] = [[0, local_ref_len]]
            else:
                pos = 0
                round_task_num = int(local_ref_len / window_size)
                for j in range(round_task_num):
                    if chrom in task_list_bychrom:
                        task_list_bychrom[chrom].append([pos, pos + window_size])
                    else:
                        task_list_bychrom[chrom] = [[pos, pos + window_size]]
                    pos += window_size

                if pos < local_ref_len:
                    if chrom in task_list_bychrom:
                        task_list_bychrom[chrom].append([pos, local_ref_len])
                    else:
                        task_list_bychrom[chrom] = [[pos, local_ref_len]]

        ## End add

    else:
        chrom = options.chrom

        ## V1.3.6 added for process a given chrom of region
        if chrom in all_possible_chrs:
            start = 0
            end = aln_file.get_reference_length(chrom)

        else:
            cords = chrom.split(':')[1]
            chrom, start, end = chrom.split(':')[0], int(cords.split('-')[0]), int(cords.split('-')[1])

        task_list_bychrom[chrom] = []

        region_length = end - start + 1

        if region_length < window_size:
            task_list_bychrom[chrom].append([start, end])

        else:
            pos = 0
            round_task_num = int(region_length / window_size)
            for j in range(round_task_num):
                task_list_bychrom[chrom].append([pos, pos + window_size])
                pos += window_size

            if pos < region_length:
                task_list_bychrom[chrom].append([pos, region_length])

        ## END add.

    ## v1.3.6 added for handling task errors
    if len(task_list_bychrom) == 0:
        # print('[ERROR]: No mapped reads in this bam. Exit!')
        logging.error("No mapped reads in the BAM, please check your reference input!")
        exit()

    ## END add

    start_time = datetime.datetime.now()

    # clusters_out_path = os.path.join(options.out_path, 'clusters')
    segments_out_path = os.path.join(options.out_path, 'segments')
    predict_results_dir = os.path.join(work_dir, 'predict_results')
    if not os.path.exists(predict_results_dir):
        os.mkdir(predict_results_dir)
    if not os.path.exists(segments_out_path):
        os.mkdir(segments_out_path)
    if options.graph is True:
        graph_out_path = os.path.join(options.out_path, 'graphs')
        if not os.path.exists(graph_out_path):
            os.mkdir(graph_out_path)


    logging.info('\n****************** Step1 Image coding and segmentation ******************')
    chrom_split_files = {}
    process_pool = multiprocessing.Pool(processes=options.thread_num)
    pool_rets = []

    for chrom, task_list in task_list_bychrom.items():
        part_num = 0
        if chrom not in chrom_split_files.keys():
            chrom_split_files[chrom] = ""

        for task in task_list:
            task_start, task_end = task[0], task[1]
            chrom_split_files[chrom] += os.path.join(segments_out_path, "{}.segments.{}.bed ".format(chrom, part_num))

            pool_rets.append([process_pool.apply_async(run_collection.run_detect,
                                                       (options, sample_path, chrom, part_num, task_start, task_end)), chrom, task_start, task_end])

            # run_collection.run_detect(options, sample_path, chrom, part_num, task_start, task_end)

            part_num += 1

    process_pool.close()
    process_pool.join()

    ## v1.3.6 modified merging segment signature files
    for chrom in task_list_bychrom.keys():
        # merge splited beds
        chr_segments_file = os.path.join(segments_out_path, chrom + ".segments.all.bed")
        cmd_str = 'cat {} > {}'.format(chrom_split_files[chrom], chr_segments_file)
        os.system(cmd_str)

    ## END add

    end_time_collect = datetime.datetime.now()
    cost_time = (end_time_collect - start_time).seconds
    logging.info("[Coding finished]: Collect segment signatures, Cost time: " + str(cost_time))

    logging.info('\n****************** Step2 CNN prediction ******************')
    # # begin to predict types
    def predict_one_chrom(chrom, predict_results_dir, options):
        try:
            segments_out_file = os.path.join(segments_out_path, chrom + ".segments.all.bed")
            chrom_predict_path = os.path.join(predict_results_dir, chrom + '.predict.' + 's' + str(options.min_support))

            predict = Predict(chrom, segments_out_file)
            predict.run(chrom_predict_path, options)
            return None
        except:
            error_type, error_value, error_trace = sys.exc_info()
            # print("[ERROR]: " + str(error_value) + '. ' + 'Locate At: ' + str(traceback.extract_tb(error_trace)))
            return "[ERROR]: " + str(error_value) + '. ' + 'Locate At: ' + str(traceback.extract_tb(error_trace))

    process_pool = multiprocessing.Pool(processes=max(1, int(options.thread_num / 3)))
    pool_rets = []

    # for chrom in chroms:
    #     # predict_one_chrom (chrom, predict_results_dir, options)
    #     pool_rets.append([process_pool.apply_async(predict_one_chrom, (chrom, predict_results_dir, options)), chrom])

    for chrom in task_list_bychrom.keys():
        pool_rets.append([process_pool.apply_async(predict_one_chrom, (chrom, predict_results_dir, options)), chrom])
        # predict_one_chrom(chrom, predict_results_dir, options)

    process_pool.close()
    process_pool.join()


    end_time_predict = datetime.datetime.now()
    cost_time = (end_time_predict - end_time_collect).seconds
    logging.info("[Prediction finished]: Predicting types, Cost time: " + str(cost_time))

    # begin to score SV and merge chrom's results together
    all_scores = cal_scores_max_min(predict_results_dir)

    try:
        max_score, min_score = np.max(all_scores), np.min(all_scores)
        merged_vcf_path = os.path.join(options.out_path,
                                       "{0}.svision.s{1}.vcf".format(options.sample, options.min_support))

        # merge_split_vcfs(predict_results_dir, merged_vcf_path, max_score, min_score, chroms, options)
        merge_split_vcfs(predict_results_dir, merged_vcf_path, max_score, min_score, list(task_list_bychrom.keys()), options)

        # # stats graphs file.
        if options.graph:
            logging.info('\n****************** Step3 Computing graphs ******************')
            graph_out_path = os.path.join(options.out_path, 'graphs')
            collect_csv_same_format(graph_out_path, merged_vcf_path, options)

            # rm region folders
            for file in os.listdir(graph_out_path):

                file_path = os.path.join(graph_out_path, file)
                if os.path.isdir(file_path):
                    shutil.rmtree(file_path)

            # # v1.3.5 add time consumption
            end_time_graph = datetime.datetime.now()
            cost_time = (end_time_graph - end_time_predict).seconds

            os.remove(merged_vcf_path)
            logging.info("[Graph creation finished] Generate graphs, Cost time: " + str(cost_time))

        # rm tmp folder
        # shutil.rmtree(clusters_out_path)

        # log_file.close()

        end_time_final = datetime.datetime.now()
        cost_time = (end_time_final - start_time).seconds
        logging.info("[All steps finished] Total Cost time: " + str(cost_time) + "s")

        if not options.debug:
            shutil.rmtree(segments_out_path)
            shutil.rmtree(predict_results_dir)

    except ValueError:
        print('Empty output in the score file!!! Program exit')
        exit()



    # the following: run repeatmasker and trf to annotate TE and VNTR, non use for current version, but in v2.0
    # # analyse mechanism
    # if options.mechanism == True:
    #     print("[Additional Func: Start]: Starting ananlyze mechanim......" )
    #     mechanism_dir = os.path.join(work_dir, 'mechanism')
    #     if not os.path.exists(mechanism_dir):
    #         os.mkdir(mechanism_dir)
    #     analyze_mechanism(merged_vcf_path, mechanism_dir, options)

    # cluster original callset if required

    # if options.cluster:
    #     logging.info("[Additional Func: cluster] Starting cluster original callset......")
    #     cluster_out_file = os.path.join(work_dir, "{0}.svision.s{1}.clusterd.vcf".format(options.sample, options.min_support))
    #     cluster_original_callset(merged_vcf_path, work_dir, sample_path, cluster_out_file)

